{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load And Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and Split Train Datasets\n",
    "\n",
    "train = pd.read_csv('train.csv', delimiter=\";\")\n",
    "\n",
    "X_train = train['title'].values + train['text'].values\n",
    "Y_train = train['label'].values\n",
    "\n",
    "# Load and Split Test Datasets\n",
    "\n",
    "test = pd.read_csv('test.csv', delimiter=\";\")\n",
    "\n",
    "X_test = test['title'].values + test['text'].values\n",
    "Y_test = test['label'].values\n",
    "\n",
    "# Load and Split Evaluations Datasets\n",
    "\n",
    "eval = pd.read_csv('evaluation.csv', delimiter=\";\")\n",
    "X_eval = eval['title'].values + eval['text'].values\n",
    "Y_eval = eval['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer Data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizer Data to Sequences\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "eval_sequences = tokenizer.texts_to_sequences(X_eval)\n",
    "\n",
    "# Padding Sequences to Max Length\n",
    "\n",
    "max_length = 300\n",
    "\n",
    "pad_train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "pad_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "pad_eval_sequences = pad_sequences(eval_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_dim = 100\n",
    "Batch_size = 64\n",
    "max_len = 250\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator And Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Build Generator\n",
    "\n",
    "G = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_dim = Latent_dim),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(max_len, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Build Dicriminator\n",
    "\n",
    "D = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_dim=max_len),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discriminator History\n",
    "\n",
    "train_D_loss = []\n",
    "train_D_accuracy = []\n",
    "val_D_loss = []\n",
    "val_D_accurary = []\n",
    "\n",
    "# Generator History\n",
    "\n",
    "train_G_loss = []\n",
    "\n",
    "# Early Stopping\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 10\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Flatten, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Build GAN\n",
    "\n",
    "D.trainable = False\n",
    "gan_input = Input(shape = (G.input_shape[1],))\n",
    "fake_output = G(gan_input)\n",
    "gan_output = D(fake_output)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer = Adam(learning_rate = 0.0002), loss='binary_crossentropy')\n",
    "\n",
    "# Real Data and Fake Data for Train, Validation\n",
    "\n",
    "real_data = X_train['label' == 0]\n",
    "fake_data = X_train['label' == 1]\n",
    "\n",
    "real_val = X_test['label' == 0]\n",
    "fake_val = X_test['label' == 1]\n",
    "\n",
    "# Train GAN\n",
    "\n",
    "half_batch = Batch_size // 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Train Discriminator with Real Data:\n",
    "\n",
    "    real_idx = np.random.randint(0, real_data.shape[0], half_batch)\n",
    "    real_samples = real_data[real_idx]\n",
    "    y_real = np.zeros((half_batch, 1))\n",
    "\n",
    "    # Generate Fake Samples:\n",
    "\n",
    "    noise = np.random.normal(0, 1, (half_batch, Latent_dim))\n",
    "    fake_samples = G.predict(noise)\n",
    "    y_fake = np.ones((half_batch, 1))\n",
    "\n",
    "    # Combine Real and Fake samples for Training Discriminator\n",
    "\n",
    "    X_D = np.vstack([real_samples, fake_samples])\n",
    "    Y_D = np.vstack([y_real, y_fake])\n",
    "\n",
    "    # Shuffle the Combined Data\n",
    "\n",
    "    idx = np.random.permutation(X_D.shape[0])\n",
    "    X_D = X_D[idx]\n",
    "    Y_D = Y_D[idx]\n",
    "\n",
    "    # Train Discriminator\n",
    "\n",
    "    D.trainable = True\n",
    "    D_Loss = D.train_on_batch(X_D, Y_D)\n",
    "\n",
    "    # Train Generator\n",
    "\n",
    "    noise = np.random.normal(0, 1, (Batch_size, Latent_dim))\n",
    "    y_valid = np.zeros((Batch_size, 1))\n",
    "    G_Loss = gan.train_on_batch(noise, y_valid)\n",
    "\n",
    "    # Validation Data\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        # Get Real Validation Set\n",
    "\n",
    "        val_real_idx = np.random.randint(0, real_val.shape[0], half_batch)\n",
    "        val_real_samples = real_val[val_real_idx]\n",
    "        val_real_y = np.zeros((half_batch, 1))\n",
    "\n",
    "        # Create Fake Samples for Validation\n",
    "\n",
    "        val_noise = np.random.normal(0, 1, (half_batch, Latent_dim))\n",
    "        val_fake_samples = G.predict(val_noise)\n",
    "        val_fake_y = np.ones((half_batch, 1))\n",
    "\n",
    "        # Evaluate on Validation Set\n",
    "\n",
    "        val_X = np.vstack([val_real_samples, val_fake_samples])\n",
    "        val_Y = np.vstack([val_real_y, val_fake_y])\n",
    "\n",
    "        # Shuffle Validation Data\n",
    "\n",
    "        val_idx = np.random.permutation(val_X.shape[0])\n",
    "        val_X = val_X[val_idx]\n",
    "        val_Y = val_Y[val_idx]\n",
    "\n",
    "        # Validation Loss\n",
    "\n",
    "        D_Loss_Val = D.evaluate(val_X, val_Y, verbose = 0)\n",
    "\n",
    "        # Save Validation Metrics\n",
    "\n",
    "        val_D_loss.append(D_Loss_Val[0])\n",
    "        val_D_accurary.append(D_Loss_Val[1])\n",
    "\n",
    "        # Early Stopping based on Validation accuracy\n",
    "\n",
    "        if D_Loss_Val[1] > best_val_acc:\n",
    "            best_val_acc = D_Loss_Val[1]\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early Stopping at Epoch: {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    # Save Training Metrics\n",
    "\n",
    "    train_D_loss.append(D_Loss[0])\n",
    "    train_D_accuracy.append(D_Loss[1])\n",
    "    train_G_loss.append(G_Loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch + 1}/{epoch} [D loss: {D_Loss[0]:.4f}, acc: {D_Loss[1]:.4f}] [G loss: {G_Loss[0]:.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show and Save Plot Models Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Discriminator Loss\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_D_loss, label = \"Train\")\n",
    "plt.plot(\n",
    "    np.arange(0, len(train_D_loss), 10),\n",
    "    train_D_loss,\n",
    "    label = \"Validation\"\n",
    ")\n",
    "plt.title(\"Discriminator Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Discriminator Accuracy\n",
    "\n",
    "plt.subplot(1, 3 ,2)\n",
    "plt.plot(train_D_accuracy, label = \"Train\")\n",
    "plt.plot(\n",
    "    np.arange(0, len(train_D_accuracy), 10),\n",
    "    train_D_accuracy,\n",
    "    label = \"Validation\"\n",
    ")\n",
    "plt.title(\"Discriminator Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Generator Loss\n",
    "\n",
    "plt.subplot(1, 3 ,3)\n",
    "plt.plot(train_G_loss)\n",
    "plt.title(\"Generator Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"plot.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show and Save Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_eval = X_eval['label' == 0]\n",
    "real_eval_y = Y_eval['label' == 0]\n",
    "\n",
    "# Evaluate Discriminator\n",
    "\n",
    "real_loss, real_acc = D.evaluate(X_eval, Y_eval, verbose = 0)\n",
    "\n",
    "# Evaluate Generator\n",
    "\n",
    "noise = np.random.normal(0, 1, (len(real_eval), Latent_dim))\n",
    "eval_fake_samples = G.predict(noise)\n",
    "eval_fake_y = np.ones(len(eval_fake_samples), 1)\n",
    "fake_loss, fake_acc = D.evaluate(eval_fake_samples, eval_fake_y, verbose = 0)\n",
    "\n",
    "# Plot Evaluate Models\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Accuracy Plot\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "accuracies = [real_acc, fake_acc]\n",
    "plt.bar(['Real News', 'Generated News'], accuracies)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "for i,v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha = 'center')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "# Loss Plot\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "losses = [real_loss, fake_loss]\n",
    "plt.bar(['Real News', 'Generated News'], losses)\n",
    "plt.title('Classification Loss')\n",
    "plt.ylabel('Loss')\n",
    "for i,v in enumerate(losses):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha = 'center')\n",
    "\n",
    "# Show Plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save Plot\n",
    "\n",
    "plt.savefig('evaluate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "G.save('Generator.h5')\n",
    "D.save('Discriminator.h5')\n",
    "gan.save('GAN.h5')\n",
    "with open('tokenizer_GAN.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
