{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library and Module necessary for Model"
      ],
      "metadata": {
        "id": "edMvPVnM5slg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tổng quan mô hình**\n",
        "--\n",
        "---\n",
        "- Mô hình Naive Bayes giả định rằng các đặc trưng trong dữ liệu độc lập với nhau.\n",
        "- Với bài toán phân loại mô hình Naive Bayes đem lại một hiệu suất tốt với cách triển khai đơn giản."
      ],
      "metadata": {
        "id": "6sQHrP0m8p8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oONpi2EF05jb",
        "outputId": "7272e2f1-6485-47f7-98cf-86afad7eb9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks/nlp-getting-started/sample_submission.csv\n",
            "/content/drive/My Drive/Colab Notebooks/nlp-getting-started/test.csv\n",
            "/content/drive/My Drive/Colab Notebooks/nlp-getting-started/train.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# tensor flow module\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# matplotlib\n",
        "from matplotlib import colors\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# word vectorizor\n",
        "# first converts the text into a matrix of word counts\n",
        "# then transforms these counts by normalizing them based on the term frequency\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "for dirname, _, filenames in os.walk('/content/drive/My Drive/Colab Notebooks/nlp-getting-started'\n",
        "):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "Rk7IVC9DHCiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset\n",
        "\n",
        "---\n",
        "- Bộ dữ liệu phân loại nhị phân được dùng trong mô hình là Natural Language Processing With Disaster Tweets. [Link Drive Dataset ở đây.](https://drive.google.com/drive/folders/19bqWjvVvlYBGucs8zFWv1-xr1FjN_u_7) Hoặc có sẵn file csv trong Folder, thay đổi đường dẫn để lấy Dataset.\n",
        "- Sau khi Get Data chia Data làm 2 tập train và test."
      ],
      "metadata": {
        "id": "wKI_tNdD8bRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/nlp-getting-started/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/nlp-getting-started/test.csv\")"
      ],
      "metadata": {
        "id": "vZaiKFVE_pn2"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeding Tweet Text.\n",
        "\n",
        "---\n",
        "- Khi làm việc với dữ liệu là ngôn ngữ tự nhiên, ta phải Embedding các *câu* (sentence), *từ ngữ* (text) thành các *vector*.\n",
        "- Trong mô hình này ta dùng Tensorflow universal-sentence-encoder để Embedding."
      ],
      "metadata": {
        "id": "iACiPDBPHHu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\n",
        "X_train_embeddings = embed(train_df[\"text\"].values)\n",
        "X_test_embeddings = embed(test_df[\"text\"].values)"
      ],
      "metadata": {
        "id": "PRbCyu2dICIx"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Numpy Matrics\n",
        "---\n",
        "- Để có thể huấn luyện mô hình Naive Bayes, ta chuẩn hoá các mẫu dữ liệu thành ma trận với *hàng* (các mẫu độc lập) và *cột* (các đặc trưng) nhằm tối ưu hoá việc tổ chức, xử lý, tính toán. Với bộ dữ liệu lớn hay nhỏ đều có thể dùng được."
      ],
      "metadata": {
        "id": "IBClOz13I9gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_matrix = X_train_embeddings['outputs'].numpy()\n",
        "X_test_matrix = X_test_embeddings['outputs'].numpy()\n",
        "\n",
        "Y_train = tf.constant(train_df[\"target\"])"
      ],
      "metadata": {
        "id": "ItFlGX6NLT0L"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build And Train Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "glT7OCy6LqzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Naive Bayes Classifier\n",
        "---\n",
        "- # `fit()`: Huấn luyện học máy mô hình Naive Bayes.\n",
        "- # `predict()`: Phân loại dữ liệu dựa trên mô hình Naive Bayes."
      ],
      "metadata": {
        "id": "ohjBKhaNLzoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFNaiveBayesClassifier:\n",
        "    dist = None\n",
        "\n",
        "    # X is the matrix containing the vectors for each sentence\n",
        "    # y is the list target values in the same order as the X matrix\n",
        "    def fit(self, X, y):\n",
        "        unique_y = np.unique(y) # unique target values: 0,1\n",
        "        print(unique_y)\n",
        "        # `points_by_class` is a numpy array the size of\n",
        "        # the number of unique targets.\n",
        "        # in each item of the list is another list that contains the vector\n",
        "        # of each sentence from the same target value\n",
        "        points_by_class = np.asarray([np.asarray(\n",
        "            [np.asarray(\n",
        "                X.iloc[x,:]) for x in range(0,len(y)) if y[x] == c]) for c in unique_y])\n",
        "        mean_list=[]\n",
        "        var_list=[]\n",
        "        for i in range(0, len(points_by_class)):\n",
        "            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n",
        "            mean_list.append(mean_var)\n",
        "            var_list.append(var_var)\n",
        "        mean=tf.stack(mean_list, 0)\n",
        "        var=tf.stack(var_list, 0)\n",
        "        # Create a 3x2 univariate normal distribution with the\n",
        "        # known mean and variance\n",
        "        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
        "\n",
        "    def predict(self, X):\n",
        "        assert self.dist is not None\n",
        "        nb_classes, nb_features = map(int, self.dist.scale.shape)\n",
        "\n",
        "        # uniform priors\n",
        "        priors = np.log(np.array([1. / nb_classes] * nb_classes))\n",
        "\n",
        "        # Conditional probabilities log P(x|c)\n",
        "        # (nb_samples, nb_classes, nb_features)\n",
        "        all_log_probs = self.dist.log_prob(\n",
        "            tf.reshape(\n",
        "                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n",
        "        # (nb_samples, nb_classes)\n",
        "        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n",
        "\n",
        "        # posterior log probability, log P(c) + log P(x|c)\n",
        "        joint_likelihood = tf.add(priors, cond_probs)\n",
        "\n",
        "        # normalize to get (log)-probabilities\n",
        "        norm_factor = tf.reduce_logsumexp(\n",
        "            joint_likelihood, axis=1, keepdims=True)\n",
        "        log_prob = joint_likelihood - norm_factor\n",
        "        # exp to get the actual probabilities\n",
        "        return tf.exp(log_prob)"
      ],
      "metadata": {
        "id": "-c_ZnAH0MLC0"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Naive Bayes Model"
      ],
      "metadata": {
        "id": "kvcakJsZN589"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_nb = TFNaiveBayesClassifier()"
      ],
      "metadata": {
        "id": "u1CPY735OF7R"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Naive Bayes With Train Data"
      ],
      "metadata": {
        "id": "t8qhiQGzOMr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_nb.fit(X=pd.DataFrame(X_train_matrix), y=Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "rCl3S7JaOTMs",
        "outputId": "5f622081-3b0d-4a4b-ff6b-75724318c8b2"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index([False, False, False, False, False, False, False, False, False, False,\\n       ...\\n       False, False, False, False, False, False, False, False, False, False],\\n      dtype='bool', length=7613)] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-0212505df1ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_nb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-7aced0be6f21>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Filter X for the current class c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mclass_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Convert the filtered DataFrame to a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpoints_by_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([False, False, False, False, False, False, False, False, False, False,\\n       ...\\n       False, False, False, False, False, False, False, False, False, False],\\n      dtype='bool', length=7613)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Test Data"
      ],
      "metadata": {
        "id": "__WDliyyOfKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = tf_nb.predict(X_test_matrix)"
      ],
      "metadata": {
        "id": "C_GfO9lOOm3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}